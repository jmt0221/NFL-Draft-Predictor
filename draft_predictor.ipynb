{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns\n",
    "import textdistance\n",
    "import config #personal file containing SQL database information\n",
    "import mysql.connector\n",
    "import random\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as BS\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score \n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets us see all columns in dataframes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the chrome window with selenium\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the years of player data we want to scrape\n",
    "years = list(range(2000,2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_results(years):\n",
    "    '''\n",
    "    Grab NFL draft data for the range of years specified\n",
    "    '''\n",
    "    drafts = []\n",
    "    \n",
    "    for year in tqdm(years):\n",
    "        time.sleep(random.choice([x/10 for x in range(3,8)]))\n",
    "        driver.get(f'http://www.nfl.com/draft/history/fulldraft?season={year}&round=round1#round1')\n",
    "\n",
    "        x = driver.find_element_by_class_name(\"draft-history-table\")\n",
    "\n",
    "        names = x.text\n",
    "        names = names.split('\\n')\n",
    "\n",
    "        #drop the rows that dont contain player data\n",
    "        names = [x for x in names if '#' not in x]\n",
    "        \n",
    "        lst = []\n",
    "        for i in range(len(names)):\n",
    "            if 'ROUND' in names[i]:\n",
    "                lst.append(i)\n",
    "        lst.pop(0)\n",
    "        lst = [lst[0] , lst[1] - 1, lst[2] - 2, lst[3] - 3, lst[4] -4, lst[5] - 5]\n",
    "        names = [x for x in names if 'ROUND' not in x]\n",
    "\n",
    "        \n",
    "        #turn strings into lists\n",
    "        for i in range(len(names)):\n",
    "            names[i] = names[i].split()\n",
    "\n",
    "\n",
    "        #if the team name contains 2 words, drop the first then drop the location of all teams\n",
    "        for row in names:\n",
    "            if row[1] in ['San','New','Green','Tampa','Los','Kansas','St.']:\n",
    "                row.pop(1)\n",
    "            row.pop(1)\n",
    "\n",
    "\n",
    "        #if college name is multiple words, combine into one, \n",
    "        for row in names:\n",
    "            if len(row) == 7:\n",
    "                row[-2] = row[-2] + ' ' + row[-1]\n",
    "                row.pop()\n",
    "            if len(row) == 8:\n",
    "                row[-3] = row[-3] + ' ' + row[-2] + ' ' + row[-1]\n",
    "                row.pop()\n",
    "                row.pop()\n",
    "        \n",
    "        for player in names:\n",
    "            d = {}\n",
    "            player[0] = int(player[0])\n",
    "            d['pick'] = player[0]\n",
    "            d['team'] = player[1]\n",
    "            d['name'] = player[2] + ' ' + player[3]\n",
    "            d['position'] =  player[4]\n",
    "            d['college'] = player[5]\n",
    "            d['round'] = player[0]\n",
    "            d['year'] = year\n",
    "\n",
    "            if player[0] < lst[0]:\n",
    "                d['round'] = 1\n",
    "            elif player[0] < lst[1]:\n",
    "                d['round'] = 2\n",
    "            elif player[0] < lst[2]:\n",
    "                d['round'] = 3\n",
    "            elif player[0] < lst[3]:\n",
    "                d['round'] = 4\n",
    "            elif player[0] < lst[4]:\n",
    "                d['round'] = 5\n",
    "            elif player[0] < lst[5]:\n",
    "                d['round'] = 6\n",
    "            else:\n",
    "                d['round'] = 7\n",
    "\n",
    "            drafts.append(d)\n",
    "    return drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drafts = draft_results(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we only want the offensive positions, so we create a subset of out data\n",
    "off = ['WR','RB','QB','TE']\n",
    "off_draft = [x for x in drafts if x['position'] in off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(off_draft)\n",
    "df.to_csv('players.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to Grab player college statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_urls(years):\n",
    "    '''\n",
    "    get individual player urls\n",
    "    '''\n",
    "    urls = []\n",
    "    for year in tqdm(years):\n",
    "        time.sleep(random.choice([x/10 for x in range(4,9)]))\n",
    "        response = requests.get(f'https://www.pro-football-reference.com/years/{year}/draft.htm')\n",
    "        soup = BS(response.content, 'html.parser')\n",
    "        x = soup.select('.right a')\n",
    "        for player in x:\n",
    "            urls.append(player['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = player_urs(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def college_stats(urls)\n",
    "    #plug in urls from beautiful soup and scrape player data\n",
    "    stats = []\n",
    "    for url in tqdm(urls):\n",
    "        try:\n",
    "            time.sleep(random.choice([x/10 for x in range(4,11)]))\n",
    "            test = []\n",
    "            driver.get(url)\n",
    "            player = driver.find_element_by_id('info').text.split('\\n')\n",
    "            position = [x for x in player if 'Position' in x]\n",
    "            position = position[0].split()[1]\n",
    "            if position == 'QB':\n",
    "                #get QB passing stats\n",
    "                passing = driver.find_element_by_id('passing').text.split('\\n')\n",
    "                #get QB rushing stats\n",
    "                rushing = driver.find_element_by_id('rushing').text.split('\\n')\n",
    "                test.append([player,passing,rushing])\n",
    "\n",
    "            elif position == 'WR' or position == 'TE':\n",
    "                #get WR stats\n",
    "                rec_rushing = driver.find_element_by_id('receiving').text.split('\\n')\n",
    "                test.append([player,rec_rushing])\n",
    "\n",
    "            elif position == 'RB':\n",
    "                #get RB rushing stats\n",
    "                rushing = driver.find_element_by_id('rushing').text.split('\\n')\n",
    "                test.append([player,rushing])\n",
    "\n",
    "\n",
    "            elif position != 'P' or position != 'K':\n",
    "                #get defensive stats\n",
    "                defense = driver.find_element_by_id('defense').text.split('\\n')\n",
    "                test.append([player, defense])\n",
    "\n",
    "            lst.append(test)\n",
    "        except:\n",
    "            pass\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = college_stats(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split all the players by their positions, we will only work with wr and rb but all are avaialable\n",
    "wr = []\n",
    "qb = []\n",
    "rb = []\n",
    "de = []\n",
    "te = []\n",
    "for player in tqdm(stats):\n",
    "    position = [x for x in player[0][0] if 'Position' in x]\n",
    "    position = position[0].split()[1]\n",
    "    if  'QB' in position:\n",
    "        qb.append(player)\n",
    "    elif 'RB' in position:\n",
    "        rb.append(player)\n",
    "    elif 'WR' in position :\n",
    "        wr.append(player)\n",
    "    elif 'TE' in position:\n",
    "        te.append(player)\n",
    "    else:\n",
    "        de.append(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wr(wr):\n",
    "    '''\n",
    "    wide reciever parsing, we include the error list in case any players are missed\n",
    "    '''\n",
    "    wr_stats = []\n",
    "    erros = []\n",
    "    #wr player info parsing\n",
    "    for player in tqdm(wr):\n",
    "        d = {}\n",
    "        for row in player[0][0]:\n",
    "            d['name'] = player[0][0][0]\n",
    "            if 'School' in row:\n",
    "                d['school'] = row.split(':')[1].strip()\n",
    "            if 'Position' in row:\n",
    "                d['position'] = row.split()[1]\n",
    "            if 'lb' in row:\n",
    "                d['height'] = row.split(' ')[0][:-1]\n",
    "                d['weight'] = row.split(' ')[1]\n",
    "\n",
    "\n",
    "        #only create a row if the position is only WR\n",
    "        if d['position'] == 'WR':\n",
    "            wr_col = wr[0][0][1][1].split('Class')[1].split()\n",
    "            for i in range(3,6):\n",
    "                wr_col[i] +='_rec'\n",
    "            for i in range(6,10):\n",
    "                wr_col[i] +='_rush'\n",
    "            wr_col = wr_col[:-4]\n",
    "            print(d['name'])\n",
    "            result = [x for x in player[0][1] if x.startswith('20') or x.startswith('*') or x.startswith('199')][-1]\n",
    "            if 'SO' in result:\n",
    "                result = result.split(' SO ')[1].split()\n",
    "                d['class'] = 'SO'\n",
    "            if 'JR' in result:\n",
    "                result = result.split(' JR ')[1].split()\n",
    "                d['class'] = 'JR'\n",
    "            if 'SR' in result:\n",
    "                result = result.split(' SR ')[1].split()\n",
    "                d['class'] = 'SR'\n",
    "            try:\n",
    "                for i in range(len(wr_col)):\n",
    "                    d[wr_col[i]] = result[i]\n",
    "                wr_stats.append(d)\n",
    "            except:\n",
    "                erros.append(d)\n",
    "                pass\n",
    "    return wr_stats, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_stats, errors = clean_wr(wr)\n",
    "wr_df = pd.DataFrame(wr_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 1 player that was scraped incorrectly by script and clean weight data\n",
    "wr_df = pd.DataFrame(wr_stats)\n",
    "wr_df['weight'] = wr_df['weight'].apply(lambda x: str(x)[:3])\n",
    "wr_df = wr_df[wr_df['name'] != 'Keary Colbert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the height variable and convert to an inches\n",
    "wr_df.height = wr_df['height'].apply(lambda x: str(x))\n",
    "wr_df.height = wr_df.height.apply(lambda x: int(x.split('-')[0])*12 + int(x.split('-')[1]) if x[0] != 'n' else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert remaining strings to integers\n",
    "\n",
    "cols = ['Att_rush', 'Avg_rec', 'Avg_rush', 'G', 'Rec', 'TD_rec',\n",
    "       'TD_rush', 'Yds_rec', 'Yds_rush', 'height','weight']\n",
    "\n",
    "wr_df[cols] = wr_df[cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split missing data for later scraping \n",
    "full_wr = wr_df[~wr_df.Att_rush.isna()]\n",
    "missing_wr = wr_df[wr_df.Att_rush.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge together the previous dataframe with draft data and this player stat dataframe\n",
    "final_wr = pd.merge(full_wr,df,on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are missing some data from a few players, we need to grab the Nan's from another online source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wr = list(final_wr[final_wr.height.isna()].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_ht_wt(missing_names):\n",
    "    '''\n",
    "    get the missing player height and weight data for wrs\n",
    "    '''\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://www.pro-football-reference.com/players/I/IgleJu00.htm')\n",
    "    filled_missing = []\n",
    "    for name in tqdm(missing_names):\n",
    "        d = {}\n",
    "        time.sleep(random.choice([x/10 for x in range(4,9)]))\n",
    "        try:  \n",
    "\n",
    "            search_bar = driver.find_element_by_xpath('//*[@id=\"header\"]/div[3]/form/div/div/input[2]')\n",
    "            search_bar.send_keys(name)\n",
    "            search_bar.send_keys(Keys.ENTER)\n",
    "\n",
    "            url = driver.current_url\n",
    "            if 'search' in url:\n",
    "                try:\n",
    "                    if driver.find_element_by_xpath('//*[@id=\"players\"]/div[1]/div[1]/a').get_attribute('href'):\n",
    "                        link = driver.find_element_by_xpath('//*[@id=\"players\"]/div[1]/div[1]/a').get_attribute('href')\n",
    "                        driver.get(link)\n",
    "\n",
    "                except:\n",
    "                    time.sleep(random.choice([x/10 for x in range(2,6)]))\n",
    "                    driver.get('https://www.pro-football-reference.com/players/I/IgleJu00.htm')\n",
    "                    pass\n",
    "\n",
    "            height = driver.find_element_by_xpath('//*[@id=\"meta\"]/div/p[3]/span[1]').text\n",
    "            weight = driver.find_elements_by_xpath('//*[@id=\"meta\"]/div/p[3]/span[2]')[0].text\n",
    "            d['name'] = name\n",
    "            d['weight'] = weight\n",
    "            d['height'] = height\n",
    "\n",
    "            filled_missing.append(d)\n",
    "        except:\n",
    "            time.sleep(random.choice([x/10 for x in range(2,6)]))\n",
    "            driver.get('https://www.pro-football-reference.com/players/I/IgleJu00.htm')\n",
    "            pass\n",
    "    return filled_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wr = missing_ht_wt(missing_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ht_wt(missing):\n",
    "    '''\n",
    "    clean weight and convert height to inches\n",
    "    '''\n",
    "    for player in missing:\n",
    "        if 'lb' in player['weight']:\n",
    "            player['weight'] = int(player['weight'][:3])\n",
    "        if '-' in player['height']:\n",
    "            player['height'] = int(player['height'][0])*12 + int(player['height'][2:])\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wr = clean_ht_wt(missing_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillin_ht_wt(missing,dframe):\n",
    "    '''\n",
    "    fill the missing heights and weights\n",
    "    '''\n",
    "    for player in tqdm(missing):\n",
    "        if type(player['weight']) == int:\n",
    "            dframe.loc[dframe['name'] == player['name'],'height'] = int(player['height'])\n",
    "            dframe.loc[dframe['name'] == player['name'],'weight'] = int(player['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillin_ht_wt(missing_wr, final_wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_wr.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_wr.to_csv('final_wr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Back Statistics Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rb(rb):\n",
    "    '''\n",
    "    Running Back parsing, we include the error list in case any players are missed\n",
    "    '''\n",
    "rb_stats = []\n",
    "erros = []\n",
    "#rb player info parsing\n",
    "for player in tqdm(rb):\n",
    "    d = {}\n",
    "    for row in player[0][0]:\n",
    "        d['name'] = player[0][0][0]\n",
    "        if 'School' in row:\n",
    "            d['school'] = row.split(':')[1].strip()\n",
    "        if 'Position' in row:\n",
    "            d['position'] = row.split()[1]\n",
    "        if 'lb' in row:\n",
    "            d['height'] = row.split(' ')[0][:-1]\n",
    "            d['weight'] = row.split(' ')[1]\n",
    "    \n",
    "    \n",
    "    #only create a row if the position is only RB\n",
    "    if d['position'] == 'RB':\n",
    "        rb_col = rb[0][0][1][1].split('Class')[1].split()\n",
    "        for i in range(3,6):\n",
    "            rb_col[i] +='_rec'\n",
    "        for i in range(6,10):\n",
    "            rb_col[i] +='_rush'\n",
    "        rb_col = rb_col[:-4]\n",
    "        print(d['name'])\n",
    "        result = [x for x in player[0][1] if x.startswith('20') or x.startswith('*') or x.startswith('199')][-1]\n",
    "        if 'SO' in result:\n",
    "            result = result.split(' SO ')[1].split()\n",
    "            d['class'] = 'SO'\n",
    "        if 'JR' in result:\n",
    "            result = result.split(' JR ')[1].split()\n",
    "            d['class'] = 'JR'\n",
    "        if 'SR' in result:\n",
    "            result = result.split(' SR ')[1].split()\n",
    "            d['class'] = 'SR'\n",
    "        try:\n",
    "            for i in range(len(rb_col)):\n",
    "                d[rb_col[i]] = result[i]\n",
    "            rb_stats.append(d)\n",
    "        except:\n",
    "            erros.append(d)\n",
    "            pass\n",
    "    return rb_stats, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_stats, errors = clean_rb(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe then search for missing data and clean\n",
    "rb_df = pd.DataFrame(rb_stats)\n",
    "missing_rb = list(rb_df[rb_df['height'].isna()]['name'])\n",
    "\n",
    "#scrape clean and fill missing wr_data\n",
    "filled_rb = missing_ht_wt(missing_rb)\n",
    "filled_rb = clean_ht_wt(filled_rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in missing value in rb_df\n",
    "fillin_ht_wt(filled_rb,rb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with draft data\n",
    "final_rb = pd.merge(rb_df,df,on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean height column\n",
    "final_rb = final_rb[final_rb['height'] !='Kolb']\n",
    "final_rb.height = final_rb['height'].apply(lambda x: str(x))\n",
    "final_rb.height = final_rb.height.apply(lambda x: int(x.split('-')[0])*12 + int(x.split('-')[1]) if '-' in x else x)\n",
    "final_rb = final_rb[final_rb['height'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to intergers \n",
    "final_rb['height'] = final_rb['height'].apply(lambda x: int(x))\n",
    "final_rb['weight'] = final_rb['weight'].apply(lambda x: int(x[:3]) if type(x) == str  else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert continuous columns to ints\n",
    "cols = ['Att', 'Avg_rec', 'Avg_rush', 'G', 'Rec_rush', 'TD_rec',\n",
    "       'TD_rush', 'height','weight']\n",
    "\n",
    "final_rb[cols] = final_rb[cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rb.to_csv('final_rb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add in combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the nfl combine csv\n",
    "df = pd.read_csv('combine_data_since_2000_PROCESSED_2018-04-26.csv')\n",
    "df.columns = ['name', 'Pos', 'Ht', 'Wt', 'Forty', 'Vertical', 'BenchReps',\n",
    "       'BroadJump', 'Cone', 'Shuttle', 'Year', 'Pfr_ID', 'AV', 'Team', 'Round',\n",
    "       'Pick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rest of the columns are mainly Null, so use these following\n",
    "df = df.loc[:,['name','Forty','Vertical','BroadJump']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge together the combine and game data\n",
    "rb = pd.merge(final_rb,df,on='name')\n",
    "wr = pd.merge(final_wr,df,on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate and multicollinear columns\n",
    "rb.drop(['Unnamed: 0','Pos','Yds_rec','Yds_rush','position_x'],axis=1,inplace=True)\n",
    "wr.drop(['Unnamed: 0','Pos','Yds_rec','Yds_rush','position_x'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine wr and rb player data, then drop columns due to null values and irrelevant data\n",
    "final_players = pd.concat([wr,rb],axis=0)\n",
    "final_players.drop(['Vertical','BroadJump'],axis=1,inplace=True)\n",
    "final_players.drop(['name','pick','team'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now to aqcquire each players NCAA conference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape the colleges that correspond to each NCAA conference\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.espn.com/college-football/teams')\n",
    "\n",
    "divisions = []\n",
    "for x in driver.find_elements_by_class_name('headline')[1:]:\n",
    "    divisions.append(x.text)\n",
    "\n",
    "teams = []\n",
    "for x in driver.find_elements_by_class_name('mt7'):\n",
    "    teams.append([y.text for y in x.find_elements_by_class_name('h5')])\n",
    "\n",
    "team_div_dict = dict(zip(divisions,teams))\n",
    "\n",
    "d = {}\n",
    "for i,j in team_div_dict.items():\n",
    "    for team in j:\n",
    "        d[' '.join(team.split()[:-1])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_location_helper(strng):\n",
    "    school_scraped = list(d.keys())                                           #INT'L Locations\n",
    "    \n",
    "    loc_vals_list = list(map(lambda x: textdistance.jaro_winkler.normalized_distance(x, strng), school_scraped))\n",
    "    loc_dict = dict(list(zip(school_scraped, loc_vals_list)))\n",
    "    result = (min(loc_dict.values()), min(loc_dict, key=loc_dict.get))\n",
    "    return result[1]\n",
    "\n",
    "\n",
    "def clean_location():\n",
    "    lowered_list = [x.lower() for x in final_players['college'].unique()]\n",
    "    cache = dict(list(zip(lowered_list, [clean_location_helper(x) for x in lowered_list])))\n",
    "    final_players.college = final_players.college.str.lower()\n",
    "    final_players.college = final_players.college.map(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the schools and then map each school to a division\n",
    "clean_location()\n",
    "final_players['college'] = final_players['college'].apply(lambda x: d[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin the year column and drop unneccesary columns and others due to multicollinearity\n",
    "final_players.loc[:,'years'] = pd.cut(final_players.year,[2000,2006,2011,2016,2020])\n",
    "final_players.drop(['school','position','Rec','G','college','years'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the final dataframe to csv\n",
    "final_players.to_csv('final_players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to SQL Database and create a table\n",
    "DB_NAME = 'NFL_DRAFT'\n",
    "cnx = mysql.connector.connect(\n",
    "    host = config.host,\n",
    "    user = config.user,\n",
    "    passwd = config.passwd,\n",
    "    database = DB_NAME)\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "#create table\n",
    "query = \"\"\" CREATE TABLE players (\n",
    "            id INT NOT NULL AUTO_INCREMENT,\n",
    "            Att_rush    FLOAT(6,3),\n",
    "            Avg_rec     FLOAT(6,3),\n",
    "            Avg_rush    FLOAT(6,3),\n",
    "            G           FLOAT(6,3),\n",
    "            Rec         FLOAT(6,3),\n",
    "            TD_rec      FLOAT(6,3),\n",
    "            TD_rush     FLOAT(6,3),\n",
    "            class       VARCHAR(15),\n",
    "            height      FLOAT(6,3),\n",
    "            weight      FLOAT(6,3),\n",
    "            college     VARCHAR(50),\n",
    "            position    VARCHAR(5),\n",
    "            round       INT(5),\n",
    "            years       VARCHAR(25),\n",
    "            Forty       FLOAT(6,3),\n",
    "            PRIMARY KEY (id))\"\"\"\n",
    "cursor.execute(query)\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_lst = []\n",
    "for dic in df_dict:\n",
    "    tuple_lst.append((float(dic['Att_rush']), float(dic['Avg_rec']), float(dic['Avg_rush']), float(dic['G']),\n",
    "                    float(dic['Rec']), float(dic['TD_rec']), float(dic['TD_rush']), str(dic['class']),\n",
    "                    float(dic['height']), float(dic['weight']), str(dic['college']), str(dic['position']),\n",
    "                    int(dic['round']), str(dic['years']), float(dic['Forty'])\n",
    "                      ))\n",
    "#setup SQL database connection and update database\n",
    "cnx = mysql.connector.connect(\n",
    "    host = config.host,\n",
    "    user = config.user,\n",
    "    passwd = config.passwd,\n",
    "    database = DB_NAME\n",
    ")\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "#create INSERT statement\n",
    "stmt = '''INSERT INTO players (Att_rush, Avg_rec, Avg_rush, G, Rec, TD_rec, TD_rush,\n",
    "                            class, height, weight, college, position, round, years, Forty)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'''\n",
    "cursor.executemany(stmt, tuple_lst)\n",
    "\n",
    "#commit insert\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you reset your notebook you can load it from csv here\n",
    "#final_players = pd.read_csv('final_players.csv')\n",
    "\n",
    "#or you can load it from SQL database here\n",
    "\n",
    "# cnx = mysql.connector.connect(\n",
    "#     host = config.host,\n",
    "#     user = config.user,\n",
    "#     passwd = config.passwd,\n",
    "#     database = DB_NAME)\n",
    "\n",
    "# cursor = cnx.cursor()\n",
    "\n",
    "# cursor.execute('''SELECT * FROM players''')\n",
    "\n",
    "# final_players = pd.DataFrame(cursor.fetchall())\n",
    "# final_players.columns = [x[0].lower() for x in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_graph(cnf_matrix):\n",
    "    '''\n",
    "    Graphs the confusion matrix in a cleaner format\n",
    "    '''\n",
    "    plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "    #Add title and Axis Labels\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    #Add appropriate Axis Scales\n",
    "    class_names = set(y) #Get class labels to add to matrix\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    #Add Labels to Each Cell\n",
    "    thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "    #Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "            plt.text(j, i, cnf_matrix[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    #Add a Side Bar Legend Showing Colors\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at distribution of rounds each position was drafted in\n",
    "sns.countplot('round',data=df,hue='position');\n",
    "plt.title('Round Drafted by Position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the scatter plots of all continuous variables\n",
    "plt.style.use('ggplot')\n",
    "cols = ['Att_rush', 'Avg_rec', 'Avg_rush', 'Rec', 'TD_rec', 'TD_rush','height','weight','Forty']\n",
    "for col in cols:\n",
    "    sns.lmplot(col, 'round',data=df,hue='position')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variables\n",
    "df = pd.get_dummies(df,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['round'],axis=1)\n",
    "y = df['round'].apply(lambda x: 1 if x < 3 else 0)\n",
    "\n",
    "x_train, x_test,y_train, y_test = train_test_split(X,y,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use smote to balance the class sizes\n",
    "smt = SMOTE()\n",
    "x_train, y_train = smt.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,random_state=2)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predct and show classification report\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "confusion_matrix_graph(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#area under ROC curve\n",
    "fpr,tpr,thresh = roc_curve(y_test,y_pred)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now with grid search\n",
    "val = cross_val_score(RandomForestClassifier(n_estimators=100),X,y,cv=4)\n",
    "val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [None, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16],\n",
    "    \"max_features\": [None,4,5,6,9,10],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\" : [1, 2, 3, 5, 6],\n",
    "    \"n_estimators\" : [10, 30, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the grid searh and print out the best parameters to use\n",
    "start = time.time()\n",
    "rf_grid_search = GridSearchCV(clf,rf_param_grid,cv=3,verbose=1)\n",
    "rf_grid_search.fit(X, y)\n",
    "\n",
    "\n",
    "print(\"Testing Accuracy: {:.4}%\".format(rf_grid_search.best_score_ * 100))\n",
    "print(\"Total Runtime for Grid Search on Random Forest Classifier: {:.4} seconds\".format(time.time() - start))\n",
    "print(\"\")\n",
    "print(\"Optimal Parameters: {}\".format(rf_grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then fit according to the output\n",
    "clf = RandomForestClassifier(criterion='gini',max_depth=16,max_features=6,min_samples_leaf=6,min_samples_split=2,n_estimators=1000,random_state=2)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "confusion_matrix_graph(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresh = roc_curve(y_test,y_pred)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs the ROC Curve\n",
    "rf_probs = clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_probs)\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='mediumorchid',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='lightskyblue', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    '''\n",
    "    Plots the importance of each feature depending how often it is split on\n",
    "    '''\n",
    "    n_features = x_train.shape[1]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X.columns.values) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "plot_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "confusion_matrix_graph(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the decision tree\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire the area under ROC \n",
    "fpr,tpr,thresh = roc_curve(y_test,y_pred)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=500,learning_rate=0.05,random_state=2)\n",
    "gbc.fit(x_train,y_train)\n",
    "y_pred = gbc.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "confusion_matrix_graph(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresh = roc_curve(y_test,y_pred)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Models that need to use Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create interaction variables to allow WR and RB to have different slopes\n",
    "#only use this for logistic regression as RF alreadys takes this into account\n",
    "df_interact = deepcopy(df)\n",
    "for col in cols:\n",
    "    df_interact[col + '* position_WR'] = df[col]*df['position_WR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_interact.drop(['round','position_WR'],axis=1)\n",
    "y = df_interact['round'].apply(lambda x: 1 if x < 3 else 0)\n",
    "\n",
    "x_train, x_test,y_train, y_test = train_test_split(X,y,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use smote to balance the class sizes\n",
    "smt = SMOTE()\n",
    "x_train, y_train = smt.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoted with interactions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(C = 1e12, solver='liblinear')\n",
    "log_reg.fit(x_train,y_train)\n",
    "y_pred = log_reg.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "confusion_matrix_graph(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC\n",
    "fpr,tpr,thresh = roc_curve(y_test,y_pred)\n",
    "auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph ROC curve\n",
    "rf_probs = log_reg.predict_proba(x_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_probs)\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='mediumorchid',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='lightskyblue', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1,kernel='rbf')\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "confusion_matrix_graph(confusion_matrix(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
